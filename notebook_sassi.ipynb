{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=========================================================================================\n",
    "Topic: Deep Learning Project on MNIST Dataset\n",
    "Author: Benalouache Sassi\n",
    "Date: 19/11/2023\n",
    "=========================================================================================\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 : Shallow network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### + Using the activation functions sigmoid :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Test Loss: 0.4715, Accuracy: 77.21%\n",
      "Epoch 2/10, Test Loss: 0.3666, Accuracy: 87.37%\n",
      "Epoch 3/10, Test Loss: 0.2979, Accuracy: 89.23%\n",
      "Epoch 4/10, Test Loss: 0.2476, Accuracy: 90.64%\n",
      "Epoch 5/10, Test Loss: 0.2222, Accuracy: 91.04%\n",
      "Epoch 6/10, Test Loss: 0.2008, Accuracy: 91.64%\n",
      "Epoch 7/10, Test Loss: 0.1872, Accuracy: 91.87%\n",
      "Epoch 8/10, Test Loss: 0.1780, Accuracy: 92.31%\n",
      "Epoch 9/10, Test Loss: 0.1706, Accuracy: 92.53%\n",
      "Epoch 10/10, Test Loss: 0.1617, Accuracy: 92.79%\n"
     ]
    }
   ],
   "source": [
    "import gzip, numpy, torch\n",
    "\n",
    "# Define the neural network class\n",
    "class ShallowNet(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(ShallowNet, self).__init__()\n",
    "        # Define the hidden layer\n",
    "        self.hidden = torch.nn.Linear(input_size, hidden_size)\n",
    "        # Define the output layer\n",
    "        self.output = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply sigmoid activation function to the hidden layer\n",
    "        x = torch.sigmoid(self.hidden(x))\n",
    "        # Output layer (without activation as we're using MSE loss)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Hyperparameters\n",
    "    batch_size = 5\n",
    "    nb_epochs = 10\n",
    "    eta = 0.001\n",
    "    hidden_size = 128\n",
    "\n",
    "    # Load the MNIST dataset\n",
    "    ((data_train, label_train), (data_test, label_test)) = torch.load(gzip.open('mnist.pkl.gz'))\n",
    "\n",
    "    # Prepare data loaders for training and testing\n",
    "    train_dataset = torch.utils.data.TensorDataset(data_train, label_train)\n",
    "    test_dataset = torch.utils.data.TensorDataset(data_test, label_test)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = ShallowNet(data_train.shape[1], hidden_size, label_train.shape[1])\n",
    "    # Initialize weights uniformly for the hidden and output layers\n",
    "    torch.nn.init.uniform_(model.hidden.weight, -0.001, 0.001)\n",
    "    torch.nn.init.uniform_(model.output.weight, -0.001, 0.001)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    loss_func = torch.nn.MSELoss(reduction='sum')\n",
    "    optim = torch.optim.SGD(model.parameters(), lr=eta)\n",
    "\n",
    "    # Training and evaluation loop\n",
    "    for n in range(nb_epochs):\n",
    "        # Set model to training mode\n",
    "        model.train()\n",
    "        for x, t in train_loader:\n",
    "            # Forward pass: compute predicted outputs\n",
    "            y = model(x)\n",
    "            # Compute loss\n",
    "            loss = loss_func(t, y)\n",
    "            # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # Perform a single optimization step\n",
    "            optim.step()\n",
    "            # Clear the gradients for the next iteration\n",
    "            optim.zero_grad()\n",
    "\n",
    "        # Set the model to evaluation mode\n",
    "        model.eval()\n",
    "        test_loss = 0.\n",
    "        correct = 0\n",
    "        with torch.no_grad(): # Turn off gradients for validation, saving memory and computations\n",
    "            for x, t in test_loader:\n",
    "                y = model(x)\n",
    "                # Accumulate test loss\n",
    "                test_loss += loss_func(t, y).item()\n",
    "                # Count correct predictions\n",
    "                correct += (torch.argmax(y, 1) == torch.argmax(t, 1)).sum().item()\n",
    "\n",
    "        # Calculate average test loss and accuracy\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        accuracy = 100. * correct / len(test_loader.dataset)\n",
    "        print(f\"Epoch {n+1}/{nb_epochs}, Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### + Adding an early stopping mechanism to prevent overfitting during the training of your neural network model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Test Loss: 0.8699, Accuracy: 18.14%\n",
      "Epoch 2/10, Test Loss: 0.8539, Accuracy: 18.23%\n",
      "Epoch 3/10, Test Loss: 0.7515, Accuracy: 32.74%\n",
      "Epoch 4/10, Test Loss: 0.6575, Accuracy: 42.10%\n",
      "Epoch 5/10, Test Loss: 0.6297, Accuracy: 44.04%\n",
      "Epoch 6/10, Test Loss: 0.4121, Accuracy: 80.01%\n",
      "Epoch 7/10, Test Loss: 0.3142, Accuracy: 83.14%\n",
      "Epoch 8/10, Test Loss: 0.2601, Accuracy: 88.30%\n",
      "Epoch 9/10, Test Loss: 0.2363, Accuracy: 89.01%\n",
      "Epoch 10/10, Test Loss: 0.2248, Accuracy: 89.23%\n"
     ]
    }
   ],
   "source": [
    "import gzip, numpy, torch\n",
    "\n",
    "# Define the neural network class\n",
    "class ShallowNet(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(ShallowNet, self).__init__()\n",
    "        # Define the hidden layer\n",
    "        self.hidden = torch.nn.Linear(input_size, hidden_size)\n",
    "        # Define the output layer\n",
    "        self.output = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply sigmoid activation function to the hidden layer\n",
    "        x = torch.sigmoid(self.hidden(x))\n",
    "        # Output layer (without activation as we're using MSE loss)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Hyperparameters\n",
    "    batch_size = 64\n",
    "    nb_epochs = 10\n",
    "    eta = 0.001\n",
    "    hidden_size = 128\n",
    "    patience = 3  # Early stopping patience\n",
    "\n",
    "    # Load the MNIST dataset\n",
    "    ((data_train, label_train), (data_test, label_test)) = torch.load(gzip.open('mnist.pkl.gz'))\n",
    "\n",
    "    # Prepare data loaders for training and testing\n",
    "    train_dataset = torch.utils.data.TensorDataset(data_train, label_train)\n",
    "    test_dataset = torch.utils.data.TensorDataset(data_test, label_test)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = ShallowNet(data_train.shape[1], hidden_size, label_train.shape[1])\n",
    "    # Initialize weights uniformly for the hidden and output layers\n",
    "    torch.nn.init.uniform_(model.hidden.weight, -0.001, 0.001)\n",
    "    torch.nn.init.uniform_(model.output.weight, -0.001, 0.001)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    loss_func = torch.nn.MSELoss(reduction='sum')\n",
    "    optim = torch.optim.SGD(model.parameters(), lr=eta)\n",
    "\n",
    "    # Initialize early stopping parameters\n",
    "    best_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    # Training and evaluation loop\n",
    "    for n in range(nb_epochs):\n",
    "        # Set model to training mode\n",
    "        model.train()\n",
    "        for x, t in train_loader:\n",
    "            # Forward pass: compute predicted outputs\n",
    "            y = model(x)\n",
    "            # Compute loss\n",
    "            loss = loss_func(t, y)\n",
    "            # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # Perform a single optimization step\n",
    "            optim.step()\n",
    "            # Clear the gradients for the next iteration\n",
    "            optim.zero_grad()\n",
    "\n",
    "        # Set the model to evaluation mode\n",
    "        model.eval()\n",
    "        test_loss = 0.\n",
    "        correct = 0\n",
    "        with torch.no_grad(): # Turn off gradients for validation, saving memory and computations\n",
    "            for x, t in test_loader:\n",
    "                y = model(x)\n",
    "                # Accumulate test loss\n",
    "                test_loss += loss_func(t, y).item()\n",
    "                # Count correct predictions\n",
    "                correct += (torch.argmax(y, 1) == torch.argmax(t, 1)).sum().item()\n",
    "\n",
    "        # Calculate average test loss and accuracy\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        accuracy = 100. * correct / len(test_loader.dataset)\n",
    "        print(f\"Epoch {n+1}/{nb_epochs}, Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "        # Early Stopping Check\n",
    "        if test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "            epochs_no_improve = 0\n",
    "            # Save the best model (optional)\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### + Training and Evaluation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip, numpy, torch\n",
    "\n",
    "# Define the neural network class\n",
    "class ShallowNet(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(ShallowNet, self).__init__()\n",
    "        # Define the hidden layer\n",
    "        self.hidden = torch.nn.Linear(input_size, hidden_size)\n",
    "        # Define the output layer\n",
    "        self.output = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply sigmoid activation function to the hidden layer\n",
    "        x = torch.sigmoid(self.hidden(x))\n",
    "        # Output layer (without activation as we're using MSE loss)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Hyperparameters\n",
    "    learning_rates = [0.01, 0.001, 0.0001]\n",
    "    hidden_sizes = [32, 64, 128]\n",
    "    batch_sizes = [5, 16, 32, 64]\n",
    "\n",
    "    ((data_train, label_train), (data_test, label_test)) = torch.load(gzip.open('mnist.pkl.gz'))\n",
    "\n",
    "    for batch_size in batch_sizes:\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            torch.utils.data.TensorDataset(data_train, label_train), \n",
    "            batch_size=batch_size, \n",
    "            shuffle=True\n",
    "        )\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "            torch.utils.data.TensorDataset(data_test, label_test), \n",
    "            batch_size=1, \n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        for eta in learning_rates:\n",
    "            for hidden_size in hidden_sizes:\n",
    "                \n",
    "                \n",
    "                model = ShallowNet(data_train.shape[1], hidden_size, label_train.shape[1])\n",
    "                torch.nn.init.uniform_(model.hidden.weight, -0.001, 0.001)\n",
    "                torch.nn.init.uniform_(model.output.weight, -0.001, 0.001)\n",
    "\n",
    "                loss_func = torch.nn.MSELoss(reduction='sum')\n",
    "                optim = torch.optim.SGD(model.parameters(), lr=eta)\n",
    "\n",
    "                for n in range(10): # Number of epochs\n",
    "                    model.train()\n",
    "                    for x, t in train_loader:\n",
    "                        y = model(x)\n",
    "                        loss = loss_func(t, y)\n",
    "                        loss.backward()\n",
    "                        optim.step()\n",
    "                        optim.zero_grad()\n",
    "\n",
    "                # Evaluate after the last epoch\n",
    "                model.eval()\n",
    "                test_loss = 0.\n",
    "                correct = 0\n",
    "                with torch.no_grad():\n",
    "                    for x, t in test_loader:\n",
    "                        y = model(x)\n",
    "                        test_loss += loss_func(t, y).item()\n",
    "                        correct += (torch.argmax(y, 1) == torch.argmax(t, 1)).sum().item()\n",
    "\n",
    "                test_loss /= len(test_loader.dataset)\n",
    "                accuracy = 100. * correct / len(test_loader.dataset)\n",
    "                print(f\"Final Epoch for Batch Size={batch_size}, η={eta}, Hidden Size={hidden_size}: Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 : Deep network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### + Function using the activation functions sigmoid :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Test Loss: 0.8110, Accuracy: 19.83%\n",
      "Epoch 2/10, Test Loss: 0.7431, Accuracy: 29.90%\n",
      "Epoch 3/10, Test Loss: 0.4130, Accuracy: 67.30%\n",
      "Epoch 4/10, Test Loss: 0.2674, Accuracy: 87.13%\n",
      "Epoch 5/10, Test Loss: 0.2249, Accuracy: 83.70%\n",
      "Epoch 6/10, Test Loss: 0.1452, Accuracy: 92.36%\n",
      "Epoch 7/10, Test Loss: 0.1246, Accuracy: 93.40%\n",
      "Epoch 8/10, Test Loss: 0.1242, Accuracy: 93.46%\n",
      "Epoch 9/10, Test Loss: 0.1015, Accuracy: 94.46%\n",
      "Epoch 10/10, Test Loss: 0.0972, Accuracy: 94.60%\n"
     ]
    }
   ],
   "source": [
    "import gzip, numpy, torch\n",
    "\n",
    "# Define the deep neural network class\n",
    "class DeepNet(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(DeepNet, self).__init__()\n",
    "        self.hidden1 = torch.nn.Linear(input_size, hidden_size1)\n",
    "        self.hidden2 = torch.nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.output = torch.nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.hidden1(x))\n",
    "        x = torch.sigmoid(self.hidden2(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Set specific hyperparameters\n",
    "    eta = 0.01\n",
    "    hidden_size1, hidden_size2 = (128, 64)\n",
    "    batch_size = 5\n",
    "\n",
    "    # Load data\n",
    "    ((data_train, label_train), (data_test, label_test)) = torch.load(gzip.open('mnist.pkl.gz'))\n",
    "\n",
    "    # Prepare data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.TensorDataset(data_train, label_train), \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.TensorDataset(data_test, label_test), \n",
    "        batch_size=1, \n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # Initialize model\n",
    "    model = DeepNet(data_train.shape[1], hidden_size1, hidden_size2, label_train.shape[1])\n",
    "    torch.nn.init.uniform_(model.hidden1.weight, -0.001, 0.001)\n",
    "    torch.nn.init.uniform_(model.hidden2.weight, -0.001, 0.001)\n",
    "    torch.nn.init.uniform_(model.output.weight, -0.001, 0.001)\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    loss_func = torch.nn.MSELoss(reduction='sum')\n",
    "    optim = torch.optim.SGD(model.parameters(), lr=eta)\n",
    "\n",
    "    # Training and evaluation loop\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        for x, t in train_loader:\n",
    "            y = model(x)\n",
    "            loss = loss_func(t, y)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        test_loss = 0.\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for x, t in test_loader:\n",
    "                y = model(x)\n",
    "                test_loss += loss_func(t, y).item()\n",
    "                correct += (torch.argmax(y, 1) == torch.argmax(t, 1)).sum().item()\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        accuracy = 100. * correct / len(test_loader.dataset)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### + This part focuses on developing and analyzing a deep neural network for the MNIST dataset, the objective was to determine the impact of various hyperparameters :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with Batch Size=5, η=0.01, Hidden Layer Sizes=(64, 32)\n",
      "Final Epoch for Batch Size=5, η=0.01, Hidden Layer Sizes=(64, 32): Test Loss: 0.0933, Accuracy: 95.13%\n",
      "\n",
      "Training with Batch Size=5, η=0.01, Hidden Layer Sizes=(128, 64)\n",
      "Final Epoch for Batch Size=5, η=0.01, Hidden Layer Sizes=(128, 64): Test Loss: 0.0930, Accuracy: 94.93%\n",
      "\n",
      "Training with Batch Size=5, η=0.01, Hidden Layer Sizes=(256, 128)\n",
      "Final Epoch for Batch Size=5, η=0.01, Hidden Layer Sizes=(256, 128): Test Loss: 0.1037, Accuracy: 94.37%\n",
      "\n",
      "Training with Batch Size=5, η=0.001, Hidden Layer Sizes=(64, 32)\n",
      "Final Epoch for Batch Size=5, η=0.001, Hidden Layer Sizes=(64, 32): Test Loss: 0.8205, Accuracy: 22.99%\n",
      "\n",
      "Training with Batch Size=5, η=0.001, Hidden Layer Sizes=(128, 64)\n",
      "Final Epoch for Batch Size=5, η=0.001, Hidden Layer Sizes=(128, 64): Test Loss: 0.8214, Accuracy: 24.34%\n",
      "\n",
      "Training with Batch Size=5, η=0.001, Hidden Layer Sizes=(256, 128)\n",
      "Final Epoch for Batch Size=5, η=0.001, Hidden Layer Sizes=(256, 128): Test Loss: 0.8177, Accuracy: 22.46%\n",
      "\n",
      "Training with Batch Size=5, η=0.0001, Hidden Layer Sizes=(64, 32)\n",
      "Final Epoch for Batch Size=5, η=0.0001, Hidden Layer Sizes=(64, 32): Test Loss: 0.9005, Accuracy: 10.84%\n",
      "\n",
      "Training with Batch Size=5, η=0.0001, Hidden Layer Sizes=(128, 64)\n",
      "Final Epoch for Batch Size=5, η=0.0001, Hidden Layer Sizes=(128, 64): Test Loss: 0.9001, Accuracy: 10.04%\n",
      "\n",
      "Training with Batch Size=5, η=0.0001, Hidden Layer Sizes=(256, 128)\n",
      "Final Epoch for Batch Size=5, η=0.0001, Hidden Layer Sizes=(256, 128): Test Loss: 0.9026, Accuracy: 10.04%\n",
      "\n",
      "Training with Batch Size=16, η=0.01, Hidden Layer Sizes=(64, 32)\n",
      "Final Epoch for Batch Size=16, η=0.01, Hidden Layer Sizes=(64, 32): Test Loss: 0.0980, Accuracy: 95.09%\n",
      "\n",
      "Training with Batch Size=16, η=0.01, Hidden Layer Sizes=(128, 64)\n",
      "Final Epoch for Batch Size=16, η=0.01, Hidden Layer Sizes=(128, 64): Test Loss: 0.5639, Accuracy: 60.79%\n",
      "\n",
      "Training with Batch Size=16, η=0.01, Hidden Layer Sizes=(256, 128)\n",
      "Final Epoch for Batch Size=16, η=0.01, Hidden Layer Sizes=(256, 128): Test Loss: 0.9074, Accuracy: 10.09%\n",
      "\n",
      "Training with Batch Size=16, η=0.001, Hidden Layer Sizes=(64, 32)\n",
      "Final Epoch for Batch Size=16, η=0.001, Hidden Layer Sizes=(64, 32): Test Loss: 0.8202, Accuracy: 22.80%\n",
      "\n",
      "Training with Batch Size=16, η=0.001, Hidden Layer Sizes=(128, 64)\n",
      "Final Epoch for Batch Size=16, η=0.001, Hidden Layer Sizes=(128, 64): Test Loss: 0.8216, Accuracy: 24.43%\n",
      "\n",
      "Training with Batch Size=16, η=0.001, Hidden Layer Sizes=(256, 128)\n",
      "Final Epoch for Batch Size=16, η=0.001, Hidden Layer Sizes=(256, 128): Test Loss: 0.8142, Accuracy: 22.70%\n",
      "\n",
      "Training with Batch Size=16, η=0.0001, Hidden Layer Sizes=(64, 32)\n",
      "Final Epoch for Batch Size=16, η=0.0001, Hidden Layer Sizes=(64, 32): Test Loss: 0.9009, Accuracy: 9.79%\n",
      "\n",
      "Training with Batch Size=16, η=0.0001, Hidden Layer Sizes=(128, 64)\n",
      "Final Epoch for Batch Size=16, η=0.0001, Hidden Layer Sizes=(128, 64): Test Loss: 0.9012, Accuracy: 10.09%\n",
      "\n",
      "Training with Batch Size=16, η=0.0001, Hidden Layer Sizes=(256, 128)\n",
      "Final Epoch for Batch Size=16, η=0.0001, Hidden Layer Sizes=(256, 128): Test Loss: 0.9014, Accuracy: 10.09%\n",
      "\n",
      "Training with Batch Size=32, η=0.01, Hidden Layer Sizes=(64, 32)\n",
      "Final Epoch for Batch Size=32, η=0.01, Hidden Layer Sizes=(64, 32): Test Loss: 0.9046, Accuracy: 9.79%\n",
      "\n",
      "Training with Batch Size=32, η=0.01, Hidden Layer Sizes=(128, 64)\n",
      "Final Epoch for Batch Size=32, η=0.01, Hidden Layer Sizes=(128, 64): Test Loss: 0.9175, Accuracy: 10.84%\n",
      "\n",
      "Training with Batch Size=32, η=0.01, Hidden Layer Sizes=(256, 128)\n",
      "Final Epoch for Batch Size=32, η=0.01, Hidden Layer Sizes=(256, 128): Test Loss: 0.3059, Accuracy: 73.76%\n",
      "\n",
      "Training with Batch Size=32, η=0.001, Hidden Layer Sizes=(64, 32)\n",
      "Final Epoch for Batch Size=32, η=0.001, Hidden Layer Sizes=(64, 32): Test Loss: 0.8211, Accuracy: 19.64%\n",
      "\n",
      "Training with Batch Size=32, η=0.001, Hidden Layer Sizes=(128, 64)\n",
      "Final Epoch for Batch Size=32, η=0.001, Hidden Layer Sizes=(128, 64): Test Loss: 0.8257, Accuracy: 21.27%\n",
      "\n",
      "Training with Batch Size=32, η=0.001, Hidden Layer Sizes=(256, 128)\n",
      "Final Epoch for Batch Size=32, η=0.001, Hidden Layer Sizes=(256, 128): Test Loss: 0.8224, Accuracy: 20.46%\n",
      "\n",
      "Training with Batch Size=32, η=0.0001, Hidden Layer Sizes=(64, 32)\n",
      "Final Epoch for Batch Size=32, η=0.0001, Hidden Layer Sizes=(64, 32): Test Loss: 0.9005, Accuracy: 10.84%\n",
      "\n",
      "Training with Batch Size=32, η=0.0001, Hidden Layer Sizes=(128, 64)\n",
      "Final Epoch for Batch Size=32, η=0.0001, Hidden Layer Sizes=(128, 64): Test Loss: 0.9004, Accuracy: 10.84%\n",
      "\n",
      "Training with Batch Size=32, η=0.0001, Hidden Layer Sizes=(256, 128)\n",
      "Final Epoch for Batch Size=32, η=0.0001, Hidden Layer Sizes=(256, 128): Test Loss: 0.9018, Accuracy: 10.94%\n",
      "\n",
      "Training with Batch Size=64, η=0.01, Hidden Layer Sizes=(64, 32)\n",
      "Final Epoch for Batch Size=64, η=0.01, Hidden Layer Sizes=(64, 32): Test Loss: 0.9310, Accuracy: 10.09%\n",
      "\n",
      "Training with Batch Size=64, η=0.01, Hidden Layer Sizes=(128, 64)\n",
      "Final Epoch for Batch Size=64, η=0.01, Hidden Layer Sizes=(128, 64): Test Loss: 0.9337, Accuracy: 10.09%\n",
      "\n",
      "Training with Batch Size=64, η=0.01, Hidden Layer Sizes=(256, 128)\n",
      "Final Epoch for Batch Size=64, η=0.01, Hidden Layer Sizes=(256, 128): Test Loss: 0.9059, Accuracy: 10.24%\n",
      "\n",
      "Training with Batch Size=64, η=0.001, Hidden Layer Sizes=(64, 32)\n",
      "Final Epoch for Batch Size=64, η=0.001, Hidden Layer Sizes=(64, 32): Test Loss: 0.7739, Accuracy: 26.49%\n",
      "\n",
      "Training with Batch Size=64, η=0.001, Hidden Layer Sizes=(128, 64)\n",
      "Final Epoch for Batch Size=64, η=0.001, Hidden Layer Sizes=(128, 64): Test Loss: 0.7901, Accuracy: 25.89%\n",
      "\n",
      "Training with Batch Size=64, η=0.001, Hidden Layer Sizes=(256, 128)\n",
      "Final Epoch for Batch Size=64, η=0.001, Hidden Layer Sizes=(256, 128): Test Loss: 0.8120, Accuracy: 20.47%\n",
      "\n",
      "Training with Batch Size=64, η=0.0001, Hidden Layer Sizes=(64, 32)\n",
      "Final Epoch for Batch Size=64, η=0.0001, Hidden Layer Sizes=(64, 32): Test Loss: 0.9006, Accuracy: 10.04%\n",
      "\n",
      "Training with Batch Size=64, η=0.0001, Hidden Layer Sizes=(128, 64)\n",
      "Final Epoch for Batch Size=64, η=0.0001, Hidden Layer Sizes=(128, 64): Test Loss: 0.9012, Accuracy: 10.84%\n",
      "\n",
      "Training with Batch Size=64, η=0.0001, Hidden Layer Sizes=(256, 128)\n",
      "Final Epoch for Batch Size=64, η=0.0001, Hidden Layer Sizes=(256, 128): Test Loss: 0.9007, Accuracy: 9.79%\n"
     ]
    }
   ],
   "source": [
    "import gzip, numpy, torch\n",
    "\n",
    "# Define the deep neural network class\n",
    "class DeepNet(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(DeepNet, self).__init__()\n",
    "        # Define the first hidden layer\n",
    "        self.hidden1 = torch.nn.Linear(input_size, hidden_size1)\n",
    "        # Define the second hidden layer\n",
    "        self.hidden2 = torch.nn.Linear(hidden_size1, hidden_size2)\n",
    "        # Define the output layer\n",
    "        self.output = torch.nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply sigmoid activation function to the first hidden layer\n",
    "        x = torch.sigmoid(self.hidden1(x))\n",
    "        # Apply sigmoid activation function to the second hidden layer\n",
    "        x = torch.sigmoid(self.hidden2(x))\n",
    "        # Output layer (without activation as we're using MSE loss)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Hyperparameters\n",
    "    learning_rates = [0.01, 0.001, 0.0001]\n",
    "    hidden_sizes = [(64, 32), (128, 64), (256, 128)]  # Pairs of sizes for two hidden layers\n",
    "    batch_sizes = [5, 16, 32, 64]\n",
    "\n",
    "    ((data_train, label_train), (data_test, label_test)) = torch.load(gzip.open('mnist.pkl.gz'))\n",
    "\n",
    "    for batch_size in batch_sizes:\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            torch.utils.data.TensorDataset(data_train, label_train), \n",
    "            batch_size=batch_size, \n",
    "            shuffle=True\n",
    "        )\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "            torch.utils.data.TensorDataset(data_test, label_test), \n",
    "            batch_size=1, \n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        for eta in learning_rates:\n",
    "            for hidden_sizes_pair in hidden_sizes:\n",
    "                hidden_size1, hidden_size2 = hidden_sizes_pair\n",
    "\n",
    "                model = DeepNet(data_train.shape[1], hidden_size1, hidden_size2, label_train.shape[1])\n",
    "                torch.nn.init.uniform_(model.hidden1.weight, -0.001, 0.001)\n",
    "                torch.nn.init.uniform_(model.hidden2.weight, -0.001, 0.001)\n",
    "                torch.nn.init.uniform_(model.output.weight, -0.001, 0.001)\n",
    "\n",
    "                loss_func = torch.nn.MSELoss(reduction='sum')\n",
    "                optim = torch.optim.SGD(model.parameters(), lr=eta)\n",
    "\n",
    "                for n in range(10): # Number of epochs\n",
    "                    model.train()\n",
    "                    for x, t in train_loader:\n",
    "                        y = model(x)\n",
    "                        loss = loss_func(t, y)\n",
    "                        loss.backward()\n",
    "                        optim.step()\n",
    "                        optim.zero_grad()\n",
    "\n",
    "                # Evaluate after the last epoch\n",
    "                model.eval()\n",
    "                test_loss = 0.\n",
    "                correct = 0\n",
    "                with torch.no_grad():\n",
    "                    for x, t in test_loader:\n",
    "                        y = model(x)\n",
    "                        test_loss += loss_func(t, y).item()\n",
    "                        correct += (torch.argmax(y, 1) == torch.argmax(t, 1)).sum().item()\n",
    "\n",
    "                test_loss /= len(test_loader.dataset)\n",
    "                accuracy = 100. * correct / len(test_loader.dataset)\n",
    "                print(f\"Final Epoch for Batch Size={batch_size}, η={eta}, Hidden Layer Sizes={hidden_sizes_pair}: Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 : CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Test Loss: 0.0951, Accuracy: 97.09%\n",
      "Epoch 2/5, Test Loss: 0.0642, Accuracy: 97.96%\n",
      "Epoch 3/5, Test Loss: 0.0585, Accuracy: 98.06%\n",
      "Epoch 4/5, Test Loss: 0.0518, Accuracy: 98.53%\n",
      "Epoch 5/5, Test Loss: 0.0473, Accuracy: 98.51%\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define the CNN class\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)  # 1 input channel, 6 output channels, 5x5 kernel\n",
    "        self.pool = nn.MaxPool2d(2, 2)   # 2x2 max pooling\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5) # 6 input channels, 16 output channels, 5x5 kernel\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120) # Fully connected layer\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10) # 10 output classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4) # Flatten the tensor for the fully connected layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Main function\n",
    "if __name__ == '__main__':\n",
    "    # Hyperparameters\n",
    "    batch_size = 64\n",
    "    nb_epochs = 5\n",
    "    eta = 0.001\n",
    "\n",
    "    # Load the MNIST dataset\n",
    "    ((data_train, label_train), (data_test, label_test)) = torch.load(gzip.open('mnist.pkl.gz'))\n",
    "\n",
    "    # Convert one-hot encoded labels to class indices\n",
    "    label_train = torch.argmax(label_train, dim=1)\n",
    "    label_test = torch.argmax(label_test, dim=1)\n",
    "\n",
    "    # Reshape data to include channel dimension (1, 28, 28)\n",
    "    data_train = data_train.view(-1, 1, 28, 28)\n",
    "    data_test = data_test.view(-1, 1, 28, 28)\n",
    "\n",
    "    # Prepare data loaders for training and testing\n",
    "    train_dataset = TensorDataset(data_train, label_train)\n",
    "    test_dataset = TensorDataset(data_test, label_test)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = SimpleCNN()\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=eta)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(nb_epochs):\n",
    "        model.train()\n",
    "        for x, t in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y = model(x)\n",
    "            loss = loss_func(y, t)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for x, t in test_loader:\n",
    "                y = model(x)\n",
    "                test_loss += F.cross_entropy(y, t, reduction='sum').item()\n",
    "                pred = y.argmax(dim=1, keepdim=True)\n",
    "                correct += pred.eq(t.view_as(pred)).sum().item()\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        accuracy = 100. * correct / len(test_loader.dataset)\n",
    "        print(f\"Epoch {epoch + 1}/{nb_epochs}, Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 : Push Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Shallow network :\n",
    "\n",
    "Note : with the activation functions sigmoid and MSE Loss we have for :\n",
    "\n",
    "+ Result with Batch Size=64, η=0.01, Hidden Size=64: Test Loss: 0.9135, Accuracy: 9.21%\n",
    "\n",
    "#### Shallow network with Loss fonction Cross entropy loss, the activation function to ReLU, Xavier initialization and Adam optimizer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Test Loss: 0.1783, Accuracy: 94.71%\n",
      "Epoch 2/5, Test Loss: 0.1255, Accuracy: 96.53%\n",
      "Epoch 3/5, Test Loss: 0.1357, Accuracy: 96.34%\n",
      "Epoch 4/5, Test Loss: 0.1539, Accuracy: 96.20%\n",
      "Epoch 5/5, Test Loss: 0.1313, Accuracy: 96.47%\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import numpy\n",
    "import torch\n",
    "\n",
    "# Define the neural network class\n",
    "class ShallowNet(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(ShallowNet, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.output = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.hidden(x))  # Changed to ReLU\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    batch_size = 64\n",
    "    nb_epochs = 5\n",
    "    eta = 0.01\n",
    "    hidden_size = 64\n",
    "\n",
    "    # Load the MNIST dataset\n",
    "    ((data_train, label_train), (data_test, label_test)) = torch.load(gzip.open('mnist.pkl.gz'))\n",
    "\n",
    "    # Preprocess labels for CrossEntropyLoss\n",
    "    label_train = torch.argmax(label_train, axis=1)\n",
    "    label_test = torch.argmax(label_test, axis=1)\n",
    "\n",
    "    # Flatten the images\n",
    "    data_train = data_train.reshape(-1, 28*28)\n",
    "    data_test = data_test.reshape(-1, 28*28)\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(data_train, label_train)\n",
    "    test_dataset = torch.utils.data.TensorDataset(data_test, label_test)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    model = ShallowNet(28*28, hidden_size, 10)\n",
    "\n",
    "    # Xavier initialization\n",
    "    torch.nn.init.xavier_uniform_(model.hidden.weight)\n",
    "    torch.nn.init.xavier_uniform_(model.output.weight)\n",
    "\n",
    "    # Cross-Entropy Loss\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Adam Optimizer\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=eta)\n",
    "\n",
    "    for n in range(nb_epochs):\n",
    "        model.train()\n",
    "        for x, t in train_loader:\n",
    "            y = model(x)\n",
    "            loss = loss_func(y, t)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = 0.\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for x, t in test_loader:\n",
    "                y = model(x)\n",
    "                test_loss += loss_func(y, t).item()\n",
    "                correct += (torch.argmax(y, 1) == t).sum().item()\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        accuracy = 100. * correct / len(test_loader.dataset)\n",
    "        print(f\"Epoch {n+1}/{nb_epochs}, Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 9, True Class: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sassi\\AppData\\Local\\Temp\\ipykernel_4512\\1834315514.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_image = torch.tensor(test_image, dtype=torch.float32)  # Convert to tensor\n"
     ]
    }
   ],
   "source": [
    "# Select a test image\n",
    "test_image, true_label = data_test[8], label_test[8]\n",
    "\n",
    "# Preprocess the image\n",
    "test_image = test_image.reshape(-1, 28*28)  # Flatten the image\n",
    "test_image = torch.tensor(test_image, dtype=torch.float32)  # Convert to tensor\n",
    "\n",
    "# Make a prediction\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    prediction = model(test_image)\n",
    "\n",
    "# Get the predicted class\n",
    "predicted_class = torch.argmax(prediction).item()\n",
    "\n",
    "# Print the results\n",
    "print(f\"Predicted Class: {predicted_class}, True Class: {true_label.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Deep network :\n",
    "\n",
    "Note : with the activation functions sigmoid and MSE Loss we have for :\n",
    "\n",
    "+ Result with Batch Size=32, η=0.001, Hidden Layer Sizes=(64, 32): Test Loss: 0.8211, Accuracy: 19.64%\n",
    "\n",
    "#### Deep network with Loss fonction Cross entropy loss, the activation function to ReLU, Xavier initialization and Adam optimizer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Test Loss: 0.0046, Accuracy: 95.60%\n",
      "Epoch 2/5, Test Loss: 0.0033, Accuracy: 96.74%\n",
      "Epoch 3/5, Test Loss: 0.0030, Accuracy: 97.20%\n",
      "Epoch 4/5, Test Loss: 0.0030, Accuracy: 97.27%\n",
      "Epoch 5/5, Test Loss: 0.0027, Accuracy: 97.27%\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import numpy\n",
    "import torch\n",
    "\n",
    "class DeepNet(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(DeepNet, self).__init__()\n",
    "        self.hidden1 = torch.nn.Linear(input_size, hidden_size1)\n",
    "        self.hidden2 = torch.nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.output = torch.nn.Linear(hidden_size2, output_size)\n",
    "        self.relu = torch.nn.ReLU()  # ReLU activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.hidden1(x))\n",
    "        x = self.relu(self.hidden2(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    eta = 0.001 \n",
    "    hidden_size1, hidden_size2 = (64, 32)\n",
    "    batch_size = 32 \n",
    "\n",
    "    ((data_train, label_train), (data_test, label_test)) = torch.load(gzip.open('mnist.pkl.gz'))\n",
    "\n",
    "    label_train = torch.argmax(label_train, axis=1)\n",
    "    label_test = torch.argmax(label_test, axis=1)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.TensorDataset(data_train, label_train), \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.TensorDataset(data_test, label_test), \n",
    "        batch_size=batch_size,  # Adjusted for test loader too\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    model = DeepNet(data_train.shape[1], hidden_size1, hidden_size2, 10)\n",
    "    torch.nn.init.xavier_uniform_(model.hidden1.weight)  # Xavier initialization\n",
    "    torch.nn.init.xavier_uniform_(model.hidden2.weight)\n",
    "    torch.nn.init.xavier_uniform_(model.output.weight)\n",
    "\n",
    "    loss_func = torch.nn.CrossEntropyLoss()  # Cross entropy loss\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=eta)  # Using Adam optimizer\n",
    "\n",
    "    num_epochs = 5\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for x, t in train_loader:\n",
    "            y = model(x)\n",
    "            loss = loss_func(y, t)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = 0.\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for x, t in test_loader:\n",
    "                y = model(x)\n",
    "                test_loss += loss_func(y, t).item()\n",
    "                correct += (torch.argmax(y, 1) == t).sum().item()\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        accuracy = 100. * correct / len(test_loader.dataset)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 9, True Class: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sassi\\AppData\\Local\\Temp\\ipykernel_4512\\4142614027.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_image = torch.tensor(test_image, dtype=torch.float32)  # Convert to tensor\n"
     ]
    }
   ],
   "source": [
    "# Select a test image\n",
    "test_image, true_label = data_test[8], label_test[8]\n",
    "\n",
    "# Preprocess the image\n",
    "test_image = test_image.reshape(-1, 28*28)  # Flatten the image\n",
    "test_image = torch.tensor(test_image, dtype=torch.float32)  # Convert to tensor\n",
    "\n",
    "# Make a prediction\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    prediction = model(test_image)\n",
    "\n",
    "# Get the predicted class\n",
    "predicted_class = torch.argmax(prediction).item()\n",
    "\n",
    "# Print the results\n",
    "print(f\"Predicted Class: {predicted_class}, True Class: {true_label.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained ResNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# MNIST Dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Data Loaders\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Load Pretrained ResNet18\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 10)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n",
    "# Testing Loop\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the model on the 10000 test images: {100 * correct // total} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
